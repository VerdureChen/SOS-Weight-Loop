# Codebase of "Breaking the Spiral: A Utility-Driven Optimization Framework for Balanced Information Retrieval in the LLM Era"
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#news-and-updates">News and Updates</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#evaluation">Evaluation</a></li>
    <li><a href="#more-use-examples">More Use Examples</a></li> 
  </ol>
</details>

<!-- News and Updates -->

## News and Updates
- [03/12/2025] üíª Published code used in our experiments.



<!-- Introduction -->

## Introduction

In this study, we propose a Utility-Driven Multi-Objective Optimization (UMO) framework to effectively mitigate the ‚ÄúSpiral of Silence‚Äù. 

This framework employs a two-phase approach: an optimization phase, leveraging the NSGA-II algorithm to optimize multiple objectives simultaneously, and a memorization phase, which directly integrates these weights into the retrieval vector space without requiring additional model retraining. 

For further exploration of the ‚ÄúSpiral of Silence‚Äù phenomenon, please refer to [SOS-Retrieval-Loop](https://github.com/VerdureChen/SOS-Retrieval-Loop).


![Pipeline Structure](pipeline.png)


## Installation
<!-- 
Êàë‰ª¨ÁöÑÊñπÊ≥ïÂü∫‰∫éÊ≠§ÂâçÁöÑÂ∑•‰ΩúÔºåÂõ†Ê≠§ÊÇ®ÂèØ‰ª•Ê†πÊçÆ [SOS-Retrieval-Loop](https://github.com/VerdureChen/SOS-Retrieval-Loop)‰∏≠[Installation](https://github.com/VerdureChen/SOS-Retrieval-Loop?tab=readme-ov-file#installation)ÁöÑËØ¥ÊòéËøõË°åÂÆâË£Ö„ÄÇ
-->

Since our method is based on previous work, you may proceed with the installation by following the guidelines provided 
in the [Installation](https://github.com/VerdureChen/SOS-Retrieval-Loop?tab=readme-ov-file#installation) section of the [SOS-Retrieval-Loop](https://github.com/VerdureChen/SOS-Retrieval-Loop).
However, if you intend to run UMO independently, the **api-for-open-llm** is **not** a requisite component.

## Usage

Please see [Installation](#installation) to install the required packages.
<!-- 
Âú®ËøêË°åÊàë‰ª¨ÁöÑÊ°ÜÊû∂‰πãÂâçÔºåÊÇ®ÈúÄË¶ÅÂÖàÂêØÂä®ElasticSearch„ÄÇÂú®ÂêØÂä®ElasticSearchÊó∂ÔºåÊÇ®ÈúÄË¶ÅÂú®ÂÖ∂config/elasticsearch.ymlÊñá‰ª∂‰∏≠ËÆæÁΩÆÂ•ΩÂØπÂ∫îÁöÑhttp.portÂíåhttp.hostÔºåÂÆÉ‰ª¨Â∞ÜÁî®‰∫éÊú¨‰ªìÂ∫ì‰ª£Á†ÅËøêË°åÁöÑÈÖçÁΩÆ„ÄÇ
Âú®ÂêØÂä®api-for-open-llmÊó∂ÔºåÊÇ®ÈúÄË¶ÅÂú®.envÊñá‰ª∂‰∏≠ËÆæÁΩÆÂ•ΩPORTÔºåÂÆÉ‰πüÂ∞Ü‰Ωú‰∏∫Êú¨‰ª£Á†ÅÂ∫ìÈúÄË¶ÅÁöÑÈÖçÁΩÆ„ÄÇ
-->
Before running our framework, you need to start **ElasticSearch**. When starting **ElasticSearch**, you need to set the appropriate `http.port` and `http.host` in its `config/elasticsearch.yml` file, as these will be used for the configuration needed to run the code in this repository.



<!--
### Running the Code
ÈÄöËøá‰ª•‰∏ãÊ≠•È™§ÔºåËøêË°åÂÆûÈ™åÔºö
1. Êï∞ÊçÆÈõÜÈ¢ÑÂ§ÑÁêÜÔºö‰∏çËÆ∫ÊòØÊü•ËØ¢ËøòÊòØÊñáÊ°£ÔºåÊàë‰ª¨ÈÉΩÈúÄË¶ÅÂ∞ÜÊï∞ÊçÆÈõÜËΩ¨Âåñ‰∏∫jsonlÊ†ºÂºè„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠‰ΩøÁî®data.wikipedia_split.psgs_w100Êï∞ÊçÆÔºåÂèØÂèÇËÄÉ[DPR‰ªìÂ∫ì](https://github.com/facebookresearch/DPR?tab=readme-ov-file#resources--data-formats)ÁöÑËØ¥ÊòéÂ∞ÜÂÖ∂‰∏ãËΩΩËá≥`data_v2/raw_data/DPR`ÁõÆÂΩï‰∏ãÂπ∂Ëß£Âéã„ÄÇÊàë‰ª¨Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑËÑöÊú¨`data_v2/gen_dpr_hc_jsonl.py`ÔºåÂèØ‰ª•Â∞ÜÊï∞ÊçÆÈõÜËΩ¨Âåñ‰∏∫jsonlÊ†ºÂºèÂπ∂ÊîæÁΩÆ‰∫é`data_v2/input_data/DPR`„ÄÇÂÆûÈ™å‰∏≠‰ΩøÁî®Âà∞ÁöÑqueryÊñá‰ª∂‰Ωç‰∫é`data_v2/input_data/DPR/modified_sampled_query`„ÄÇ
   ```bash
    cd data_v2
    python gen_dpr_hc_jsonl.py 
    ```
   Âú®Êàë‰ª¨ÁöÑÊñπÊ≥ï‰∏≠ÔºåÊØè‰∏™ÊñáÊ°£ÈÉΩÂê´Êúâ‰∏Ä‰∏™ÊùÉÈáç„ÄÇÂú®ÂàùÂßãÂåñÊó∂ÔºåÂØπ‰∫épsgs_w100Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËÆæÁΩÆÊØè‰∏™ÊñáÊ°£ÁöÑÊùÉÈáç‰∏∫1„ÄÇÂèØ‰ª•Âú®`src/fake_alpha.py`‰∏≠ËÆæÁΩÆÂØπÂ∫îË∑ØÂæÑÂêéÔºå‰∏∫ÊØè‰∏™ÊñáÊ°£ËÆæÁΩÆÊùÉÈáç„ÄÇ

2. Âª∫Á´ãÊï∞ÊçÆÈõÜÁ¥¢ÂºïÔºö‰ΩøÁî®`src/retrieval_loop/run_index_builder.sh`ÔºåÈÄöËøá‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ`MODEL_NAMES`Âíå`DATA_NAMES`ÈÖçÁΩÆÔºåËÉΩÂ§ü‰∏ÄÊ¨°ÊÄßÂª∫Á´ãÊâÄÊúâÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑÁ¥¢Âºï„ÄÇ‰Ω†‰πüÂèØ‰ª•ÈÄöËøáÈÖçÁΩÆ`src/retrieval_loop/index_configs`‰∏≠configÊñá‰ª∂ÁöÑ`query_files`Âíå`output_files`Êù•Ëé∑ÂæóÂü∫‰∫éËØ•Á¥¢ÂºïÁöÑÂØπÂ∫îÊñπÊ≥ïÁöÑÊ£ÄÁ¥¢ÁªìÊûú„ÄÇÂú®Êàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠ÊâÄÊúâÊ£ÄÁ¥¢Ê®°ÂûãcheckpointÊîæÂú®`ret_model`ÁõÆÂΩï‰∏ã„ÄÇ
   ËøêË°åÔºö
   ```bash
    cd src/retrieval_loop
    bash run_index_builder.sh
   ```
3. Âú®Ëø≠‰ª£‰∏≠ËøêË°åUMOÔºö‰ΩøÁî®`src/run_weight_loop.sh`ÔºåÈÄöËøá‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ`QUERY_DATA_NAMES`ÈÖçÁΩÆÔºåËÉΩÂ§üÊâπÈáèÂåñËøêË°åÊâÄÊúâÊï∞ÊçÆÁöÑUMOÊñπÊ≥ï„ÄÇ‰Ω†ÂèØ‰ª•ÈÄöËøáËÆæÁΩÆ`TOTAL_LOOP_NUM`Êù•ÊéßÂà∂Âæ™ÁéØÊ¨°Êï∞ÔºåÁî±‰∫éÊ∂âÂèäÂà∞Á¥¢ÂºïÂ§öÊ¨°Êõ¥Êñ∞ÔºåÊØèÊ¨°Âè™ËÉΩËøêË°å‰∏Ä‰∏™Ê£ÄÁ¥¢ÊñπÊ≥ïpipeline„ÄÇÊ≥®ÊÑèÔºåËØ∑Âú®`src/run_weight_loop.sh`‰∏≠ÊØèÊ¨°ËøêË°å`../rewrite_configs.py`Êó∂ÈÖçÁΩÆÂ•ΩÂèÇÊï∞„ÄÇÂØπ‰∫é‰∏çÈúÄË¶ÅÈöèËø≠‰ª£ÊîπÂèòÁöÑÂèÇÊï∞ÔºåÂèØ‰ª•Âú®`src/retrieval_loop`‰∏≠ÁöÑÂêÑ‰∏™configÁõÆÂΩï‰∏ãÁöÑÊñá‰ª∂‰∏≠ÈÖçÁΩÆ„ÄÇ
    ËøêË°åÔºö
    ```bash
    cd src
    bash run_weight_loop.sh
    ```
-->

### Running the Code
To run our framework, follow these steps:
1. Dataset Preprocessing: Whether it is a query or a document, we need to convert the dataset to jsonl format. In our experiments, we use the data.wikipedia_split.psgs_w100 dataset, which can be downloaded to the `data_v2/raw_data/DPR` directory and unzipped according to the instructions in the [DPR repository](https://github.com/facebookresearch/DPR?tab=readme-ov-file#resources--data-formats). We provide a simple script `data_v2/gen_dpr_hc_jsonl.py`, which can convert the dataset to jsonl format and place it in `data_v2/input_data/DPR`. The query files used in the experiment are located in `data_v2/input_data/DPR/modified_sampled_query`.
   ```bash
    cd data_v2
    python gen_dpr_hc_jsonl.py 
    ```
    In our method, each document has a weight. At initialization, for the psgs_w100 dataset, we set the weight of each document to 1. You can set the weight for each document by setting the corresponding path in `src/fake_alpha.py` and running the script.
2. Build Index: Use `src/retrieval_loop/run_index_builder.sh`, by modifying the `MODEL_NAMES` and `DATA_NAMES` configuration in the file, you can build indexes for all data and models at once. You can also obtain the retrieval results based on this index for the corresponding method by configuring `query_files` and `output_files` in the config files in `src/retrieval_loop/index_configs`. In our experiments, all retrieval model checkpoints are placed in the `ret_model` directory.
   Run:
   ```bash
    cd src/retrieval_loop
    bash run_index_builder.sh
   ```
3. Run UMO in Iterations: Use `src/run_weight_loop.sh`, by modifying the `QUERY_DATA_NAMES` configuration in the file, you can run the UMO method for all data in batches. You can control the number of loops by setting `TOTAL_LOOP_NUM`. Since it involves multiple index updates, only one retrieval method pipeline can be run at a time. Please configure the parameters following `../rewrite_configs.py` in `src/run_weight_loop.sh`. For parameters that do not need to change with iterations, you can configure them in the files in the config directories in `src/retrieval_loop`.
    Then run:
    ```bash
    cd src
    bash run_weight_loop.sh
    ```
<!--
## Evaluation
Èù¢ÂêëÂÆûÈ™å‰∏≠ÁîüÊàêÁöÑÂ§ßÈáèÊï∞ÊçÆÔºå‰∏∫‰∫ÜÊîØÊåÅÊâπÈáèÂåñÁöÑËØÑ‰º∞ÊñπÊ≥ïÔºåËÆæÁΩÆ`src/evaluation/run_context_eva.sh`ËÑöÊú¨‰∏≠ÁöÑ`QUERY_DATA_NAMES`Âíå`RESULT_NAMES`ÂêéÔºåÂèØ‰ª•ËøõË°å`TASK`ËØÑ‰º∞Ôºö
1. `TASK="retrieval"`ÔºöÂØπÊØèÊ¨°Ëø≠‰ª£ÁöÑÊ£ÄÁ¥¢ÂíåÈáçÊéíÂ∫èÁªìÊûúËøõË°åËØÑ‰º∞ÔºåÂåÖÊã¨Acc@5ÂíåAcc@20„ÄÇ
2. `TASK="bleu"`ÔºöËÆ°ÁÆóÊØèÊ¨°Ëø≠‰ª£‰∏ä‰∏ãÊñáÁöÑSELF-BLEUÂÄº„ÄÇ
3. `TASK="llm_text_type_rate"`ÔºöËÆ°ÁÆóÊØèÊ¨°Ëø≠‰ª£Ê£ÄÁ¥¢ÁªìÊûú‰∏≠ÂêÑLLMÂíå‰∫∫Á±ªÁîüÊàêÊñáÊú¨ÁöÑÁôæÂàÜÊØî„ÄÇ
4. `TASK="correctness_query"`ÔºöËÆ°ÁÆóÂú®ÊØèÊ¨°Ëø≠‰ª£ÁªìÊùüÊó∂Ê£ÄÁ¥¢ÁªìÊûú‰∏≠ÂåÖÂê´‰∏çÂêåÊï∞ÈáèÊ≠£Á°ÆÊñáÊ°£ÁöÑÊü•ËØ¢‰∏™Êï∞„ÄÇ
ËØÑ‰º∞ÂêéÁîüÊàêÁöÑÁªìÊûúÊñá‰ª∂ÈªòËÆ§Â≠ò‰∫éÂØπÂ∫î`RESULT_DIR/RESULT_NAME/QUERY_DATA_NAME/results`ÁõÆÂΩï‰∏ã„ÄÇ
-->
## Evaluation
To support batch evaluation methods for the large amount of results in the experiments, after setting `QUERY_DATA_NAMES` and `RESULT_NAMES` in the `src/evaluation/run_context_eva.sh` script, you can perform `TASK` evaluation:
1. `TASK=("retrieval")`: Evaluate the retrieval results of each iteration, including Acc@5 and Acc@20.
2. `TASK=("bleu")`: Calculate the SELF-BLEU value of the context for each iteration.
3. `TASK=("llm_text_type_rate")`: Calculate the percentage of text generated by each LLM and human in the retrieval results for each iteration.
4. `TASK=("correctness_query")`: Calculate the number of queries containing different numbers of correct documents in the retrieval results at the end of each iteration.

The results generated after evaluation are stored by default in the corresponding `RESULT_DIR/RESULT_NAME/QUERY_DATA_NAME/results` directory.


<!--
## More Usage
### Checkpoint
Âú®Êàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠Ôºå‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞ËØÜÂà´Ê£ÄÁ¥¢ÁªìÊûú‰∏≠Êù•Ê∫ê‰∫éLLMÁöÑÊñáÊú¨„ÄÇÊàë‰ª¨Âü∫‰∫éNQÔºåWebQAÔºåTriviaQAÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂíåLLMÊ†πÊçÆËÆ≠ÁªÉÈõÜÊü•ËØ¢ÁîüÊàêÁöÑÊÆµËêΩÈáçÊñ∞ËÆ≠ÁªÉ‰∫Ü[Hello-SimpleAI/chatgpt-qa-detector-roberta](https://huggingface.co/Hello-SimpleAI/chatgpt-qa-detector-roberta)ÔºåËØ∑ËÆøÈóÆ[Xiaoyang221029/fine-tuned-chatgpt-qa-detector-roberta](https://huggingface.co/Xiaoyang221029/fine-tuned-chatgpt-qa-detector-roberta)‰∏ãËΩΩÂíå‰ΩøÁî®ËØ•Ê®°Âûã„ÄÇ
### ‰ªéÁ¥¢ÂºïÂà†Èô§Áõ∏Â∫îÊñáÊ°£
Áî±‰∫éÊàë‰ª¨ÁöÑÂÆûÈ™åÊ∂âÂèäÂà∞Á¥¢ÂºïÁöÑÂä®ÊÄÅÊõ¥Êñ∞ÔºåÊàë‰ª¨‰∏çÂèØËÉΩÂú®ÊØèÊ¨°Ê®°Êãü‰∏≠ÈáçÊñ∞‰ªéÈõ∂ÊûÑÈÄ†Á¥¢Âºï„ÄÇÁõ∏ÂèçÂú∞ÔºåÂú®ÊØèÊ¨°Ê®°ÊãüÊó∂ÔºåÊàë‰ª¨ÈÉΩ‰ºöÂ∞ÜÊñ∞Â¢ûÊñáÊú¨IDËÆ∞ÂΩïÂú®`src/run_logs`‰∏≠ÂØπÂ∫îÊ≠§Ê¨°ÂÆûÈ™åÁöÑ`index_add_logs`ÁõÆÂΩï‰∏ãÔºåÂæÖÂÆûÈ™åÁªìÊùüÂêéÔºåÊàë‰ª¨ÈÄöËøá`src/post_process/delete_doc_from_index.py`ËÑöÊú¨Âà†Èô§Á¥¢Âºï‰∏≠ÁöÑÁõ∏Â∫îÊñáÊ°£„ÄÇ
ÂΩìÈúÄË¶ÅÂà†Èô§FaissÁ¥¢Âºï‰∏≠ÁöÑÊñáÊ°£Êó∂ÔºåËøêË°åÔºö
```bash
cd src/post_process
python delete_doc_from_index.py --config_file_path delete_configs/delete_config_pickles.json
```
ÂÖ∂‰∏≠Ôºå`src/post_process/delete_configs/delete_config_pickles.json`ÊòØÂØπÂ∫îÈÖçÁΩÆÊñá‰ª∂ÔºåÂ∞Ü`"pickle_file_path"`Ôºå`"id_files"`‰ª•Âèä`"delete_log_path"`ËÆæÁΩÆ‰∏∫ÂØπÂ∫îÁöÑË∑ØÂæÑ„ÄÇ
-->
## More Usage
### Checkpoint
In our experiments, to better identify the text from LLM in the retrieval results, we retrained [Hello-SimpleAI/chatgpt-qa-detector-roberta](https://huggingface.co/Hello-SimpleAI/chatgpt-qa-detector-roberta) based on the training data of NQ, WebQA, TriviaQA, and the paragraphs generated by LLM according to the training set queries. Please visit [Xiaoyang221029/fine-tuned-chatgpt-qa-detector-roberta](https://huggingface.co/Xiaoyang221029/fine-tuned-chatgpt-qa-detector-roberta) to download and use this model.
### Delete Corresponding Documents from the Index
Since our experiments involve dynamic updates to the index, it is not possible to reconstruct the index from scratch in each simulation. Instead, in each simulation, we record the newly added text IDs in the `index_add_logs` directory corresponding to this experiment in `src/run_logs`. After the experiment ends, we delete the corresponding documents from the index using the `src/post_process/delete_doc_from_index.py` script.
To delete documents from the Faiss index, run:
```bash
cd src/post_process
python delete_doc_from_index.py --config_file_path delete_configs/delete_config_pickles.json
```
where `src/post_process/delete_configs/delete_config_pickles.json` is the corresponding configuration file. Set `"pickle_file_path"`, `"id_files"`, and `"delete_log_path"` to the corresponding paths.

